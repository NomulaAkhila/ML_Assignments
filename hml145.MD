<html>
<body>
<head>
<title>Regressions</title>
</head>
<H2>RIDGE REGRESSION::</H2>

Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When
multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from
the true value.
Multicollinearity, or collinearity, is the existence of near-linear relationships among the independent variables. 
Ridge Regression can be used to find new line that does not fit the training data.
or in other words introducing a small amount of bias into how new line is fit into data.

<H3>Effects of Multicollinearity::</H3>

Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of the
regression coefficients, deflate the partial t-tests for the regression coefficients, give false, nonsignificant, pvalues, and degrade the predictability of the models.

<H3>Ridge Regression Models::</H3>

Following the usual notation, suppose our regression equation is written in matrix form as
Y = XB + e
where Y is the dependent variable, X represents the independent variables, B is the regression coefficients to be
estimated, and e represents the errors are residuals.
 
<H3>Assumptions::</H3>

The assumptions are the same as those used in regular multiple regression: linearity, constant variance (no
outliers), and independence. Since ridge regression does not provide confidence limits, normality need not be assumed.

<H2>LASSO REGRESSION::</H2>

<br>Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable.</br>
<br>The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero.
Variables with a regression coefficient equal to zero after the shrinkage process are excluded from the model.</br> 
<br>Variables with non-zero regression coefficients variables are most strongly associated with the response variable. Explanatory variables can be either quantitative, categorical or both. In this session, you will apply and interpret a lasso regression analysis. 
To test a lasso regression model, you will need to identify a quantitative response variable from your data set if you haven’t already done so, and choose a few additional quantitative and categorical predictor variables to develop a larger pool of predictors.</br>

<H2>ELASTIC NET REGRESSION::</H2>

<br>Elastic net regularization. In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.</br>
<br>Elastic Net first emerged as a result of critique on lasso, whose variable selection can be too dependent on data and thus unstable. The solution is to combine the penalties of ridge regression and lasso to get the best of both worlds.</br>
</body>
</html>











